{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb994d2d-3c5e-4222-b3b4-a1a7bfcfa5fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"fs.s3a.access.key\", \"\")\n",
    "spark.conf.set(\"fs.s3a.secret.key\", \"\")\n",
    "spark.conf.set(\"fs.s3a.endpoint\", \"s3.amazonaws.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4987bced-9f36-41a6-a00f-6905aef4d80f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\nExecution Time output data generation: 13.01 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import row_number, current_timestamp, col, hour\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "def generate_output_data(ref_data, validated_data, symbol_data, files_per_day, date):\n",
    "    spark = validated_data.sparkSession  \n",
    "\n",
    "    (\n",
    "        currency_data, exchange_data, order_types_data, sides_data,\n",
    "        transaction_types_data, order_statuses_data, mics_data\n",
    "    ) = ref_data\n",
    "    \n",
    "    # Join validated_data with reference tables\n",
    "    output_data = (\n",
    "        validated_data\n",
    "        .join(transaction_types_data.selectExpr(\"transaction_type\", \"transaction_type_id\"), \"transaction_type\", \"left\")\n",
    "        .join(mics_data.selectExpr(\"mic_code\", \"mic_id\"), \"mic_code\", \"left\")\n",
    "        .join(order_statuses_data.selectExpr(\"order_status\", \"order_status_id\"), \"order_status\", \"left\")\n",
    "        .join(sides_data.selectExpr(\"side\", \"side_id\"), \"side\", \"left\")\n",
    "        .join(order_types_data.selectExpr(\"order_type_name as order_type\", \"order_type_id\"), \"order_type\", \"left\")\n",
    "        .join(currency_data.selectExpr(\"currency_name\", \"currency_id\"), \"currency_name\", \"left\")\n",
    "        .join(exchange_data.selectExpr(\"exchange_code\", \"exchange_id\"), \"exchange_code\", \"left\")\n",
    "    )\n",
    "\n",
    "    # Add transaction_hour column from transaction_timestamp\n",
    "    output_data = output_data.withColumn(\"transaction_hour\", hour(col(\"transaction_timestamp\")))\n",
    "\n",
    "    # Define column order\n",
    "    column_order = [\n",
    "        \"transaction_id\", \"transaction_parent_id\",\n",
    "        \"transaction_timestamp\", \"transaction_hour\", \"transaction_type_id\", \"mic_id\",\n",
    "        \"order_status_id\", \"side_id\", \"order_type_id\", \"symbol\",\n",
    "        \"isin\", \"price\", \"quantity\",\"adv30\", \"trader_id\", \"broker_id\",\n",
    "        \"exchange_id\", \"currency_id\",\n",
    "        \"creation_time\", \"last_update_time\", \"validation_flag\"\n",
    "    ]\n",
    "\n",
    "    # Add timestamps\n",
    "    output_data = (\n",
    "        output_data\n",
    "        .withColumn(\"creation_time\", current_timestamp())\n",
    "        .withColumn(\"last_update_time\", current_timestamp())\n",
    "        .select(*column_order)\n",
    "    )\n",
    "\n",
    "    # output_data = output_data.repartitionByRange(files_per_day, \"transaction_internal_id\")\n",
    "    output_data = output_data.repartition(files_per_day)\n",
    "    output_path = \"output_data\"\n",
    "    print(output_data.count())\n",
    "    return output_data\n",
    "\n",
    "start_time = time.time()\n",
    "# Load DataFrames from Temporary Views\n",
    "validated_data = spark.sql(\"SELECT * FROM global_temp.validated_data_view\")\n",
    "symbol_data = spark.sql(\"SELECT * FROM global_temp.symbol_data_view\")\n",
    "currency_data = spark.sql(\"SELECT * FROM global_temp.currency_data_view\")\n",
    "exchange_data = spark.sql(\"SELECT * FROM global_temp.exchange_data_view\")\n",
    "order_types_data = spark.sql(\"SELECT * FROM global_temp.order_types_data_view\")\n",
    "sides_data = spark.sql(\"SELECT * FROM global_temp.sides_data_view\")\n",
    "transaction_types_data = spark.sql(\"SELECT * FROM global_temp.transaction_types_data_view\")\n",
    "order_statuses_data = spark.sql(\"SELECT * FROM global_temp.order_statuses_data_view\")\n",
    "mics_data = spark.sql(\"SELECT * FROM global_temp.mics_data_view\")\n",
    "ref_data = (\n",
    "    currency_data,\n",
    "    exchange_data,\n",
    "    order_types_data,\n",
    "    sides_data,\n",
    "    transaction_types_data,\n",
    "    order_statuses_data,\n",
    "    mics_data,\n",
    ")\n",
    "# os.makedirs(\"output_data\", exist_ok=True)\n",
    "output_data = generate_output_data(ref_data, validated_data,symbol_data, 20, \"2025-03-17\")\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate execution time\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution Time output data generation: {execution_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03263abc-0db0-4fd6-a393-ce4d16fc8361",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, date_format\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Extract date and hour dynamically from transaction_timestamp\n",
    "output_data = output_data.withColumn(\"date\", date_format(col(\"transaction_timestamp\"), \"yyyy-MM-dd\")) \\\n",
    "                         .withColumn(\"hour\", date_format(col(\"transaction_timestamp\"), \"HH\"))\n",
    "\n",
    "# Write data partitioned by date and hour to S3 Delta Lake\n",
    "output_data.write.format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .partitionBy(\"date\", \"hour\") \\\n",
    "    .option(\"path\", \"s3://delta-lake-etl/output_data_delta/\") \\\n",
    "    .save()\n",
    "\n",
    "# Optimize with Z-Ordering\n",
    "# delta_table = DeltaTable.forPath(spark, \"s3://delta-lake-etl/output_data_delta/\")\n",
    "# delta_table.optimize().executeZOrderBy(\"transaction_timestamp\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Load Notebook",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
