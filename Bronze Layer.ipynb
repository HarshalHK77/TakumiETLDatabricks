{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Stop existing Spark session if it exists\n",
    "existing_spark = SparkSession.getActiveSession()\n",
    "if existing_spark:\n",
    "    existing_spark.stop()\n",
    "\n",
    "# Initialize new Spark session with Delta Lake support\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Takumi ETL\") \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .config(\"spark.driver.memory\", \"5g\") \\\n",
    "    .config(\"spark.executor.memory\", \"3g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3482d934-faae-4cb0-84d3-cc869357edb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"fs.s3a.access.key\", \"AKIAUMYCIDRW5XYRGE27\")\n",
    "spark.conf.set(\"fs.s3a.secret.key\", \"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\")\n",
    "spark.conf.set(\"fs.s3a.endpoint\", \"s3.amazonaws.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ee0ac83-95eb-4719-b413-d1ba7283291f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 16.229655027389526 seconds\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, to_date\n",
    "from pyspark.sql.types import *\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "source_s3_path = \"s3a://input-data-bronze-layer-bucket/input_data/\"\n",
    "target_s3_path = \"s3a://delta-lake-etl/input_delta_table/\"\n",
    "\n",
    "# Read data from source S3 bucket using Auto Loader (cloudFiles)\n",
    "df = (\n",
    "    spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"csv\")  # Change to \"parquet\" if needed\n",
    "    .option(\"cloudFiles.schemaLocation\", \"/mnt/checkpoints/input_data\")  # Schema tracking\n",
    "    .load(source_s3_path)\n",
    ")\n",
    "\n",
    "# Add partition column (Extract date from timestamp)\n",
    "df = df.withColumn(\"transaction_date\", to_date(col(\"transaction_timestamp\")))\n",
    "\n",
    "# Write data to Delta Table in append mode with partitioning\n",
    "query = (\n",
    "    df.writeStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"checkpointLocation\", \"/mnt/checkpoints/input_data\")  # Required for state tracking\n",
    "    .option(\"mergeSchema\", \"true\")  # Handles schema evolution\n",
    "    .partitionBy(\"transaction_date\")  # Partition by date\n",
    "    .outputMode(\"append\")  # Append to the Delta table\n",
    "    .trigger(once=True)  # Process existing files and stop\n",
    "    .start(target_s3_path)\n",
    ")\n",
    "\n",
    "# Wait for the streaming query to finish\n",
    "query.awaitTermination()\n",
    "\n",
    "# Optimize and Compact Delta Table (Without Z-Ordering)\n",
    "from delta.tables import DeltaTable\n",
    "delta_table = DeltaTable.forPath(spark, target_s3_path)\n",
    "delta_table.optimize().executeCompaction()  # Open-source Delta Lake compaction\n",
    "\n",
    "# End time\n",
    "end = time.time()\n",
    "print(f\"Time taken: {end - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5009fde-a1c9-483f-9ebb-08e3ac6b751b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 15.884464502334595 seconds\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "source_s3_path = \"s3a://input-data-bronze-layer-bucket/reference_market_data/\"\n",
    "target_s3_path = \"s3a://delta-lake-etl/reference_market_data/\"\n",
    "\n",
    "\n",
    "# Read data from source S3 bucket using cloudFiles (CSV or Parquet)\n",
    "df = (\n",
    "    spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"csv\")  # Change to \"parquet\" if needed\n",
    "    .option(\"cloudFiles.schemaLocation\", \"/mnt/checkpoints/reference_market_data\")  # Schema tracking\n",
    "    .load(source_s3_path)\n",
    ")\n",
    "\n",
    "# Write data to Delta Table in batch mode and store it in the target S3 bucket\n",
    "query = (\n",
    "    df.writeStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"checkpointLocation\", \"/mnt/checkpoints/reference_market_data\")  # Required for state tracking\n",
    "    .option(\"mergeSchema\", \"true\")  # Handles schema evolution\n",
    "    .outputMode(\"append\")  # Append to the Delta table\n",
    "    .trigger(once=True)  # This ensures job stops after processing existing files\n",
    "    .start(target_s3_path)  # Write processed data to the target S3 bucket in Delta format\n",
    ")\n",
    "\n",
    "# Wait for the streaming query to finish\n",
    "query.awaitTermination()\n",
    "\n",
    "# End time\n",
    "end = time.time()\n",
    "\n",
    "# Print elapsed time\n",
    "print(f\"Time taken: {end - start} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Bronze Layer",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
